{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a54effa-c481-4dac-a564-07f5f06f6aff",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron\n",
    "This notebook contains an MLP implementation from scratch without using a deep learning library.\\\n",
    "Only Numpy is used as third-party library for mathematical usage including vectors. \\\n",
    "The source code written by the guadiance of Neural Networks and Deep Learning book written by Michael Nielsen. \\\n",
    "Additionally, the MNIST dataset is utilized to train and test the model for handwritten digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31be7b9c-ad9a-4884-bd75-8e2af2f20d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11179dc-314e-410e-8434-08892d2a1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.layer_num = len(sizes) # number of layers\n",
    "        self.sizes = sizes # size of layers respectively\n",
    "        self.biases = [np.random.rand(y,1) for y in sizes[:1]] # random initialized biases for each layer except input\n",
    "        self.weights = [np.random.rand(y, x) for x, y in zip(sizes[:-1], sizes[1:])] # random initialized weights \n",
    "\n",
    "    \"\"\"\n",
    "        Return the output of the network if 'a' is input\n",
    "    \"\"\"\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    \"\"\" \n",
    "        Train the network by using mini-batch Stochastic Gradient Descent \n",
    "        First, shuffles the dataset and divides into mini-batches regarding the mini_batch_size arg.\n",
    "        Then, applies SGD to each batch by calling mini_batch_update func. which updates weights and biases\n",
    "    \"\"\"\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "        if test_data:\n",
    "            print(\"Epoch {0} complete: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    \"\"\"\n",
    "        Updates weights and biases by using backpropagation to a single mini batch\n",
    "    \"\"\"\n",
    "    def mini_batch_update(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+db for nb, db in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dw for nw, dw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw for w, nw in zip(self.weights, delta_nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb for b, nb in zip(self.biases, delta_nabla_b)]\n",
    "        \n",
    "    \"\"\"\n",
    "        This is where the magic happens, still don't know what's going on here; total black box for me.\n",
    "        As far as I understood, backpropagation is an expression for the partial derivative of the cost\n",
    "        function C with respect to any weight or bias in the network.\n",
    "        Computes the gradient of the cost function using SGD\n",
    "    \"\"\"\n",
    "    def backprop():\n",
    "        ...\n",
    "    \n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f55cb-d7f9-4400-9bac-c426397b5487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
